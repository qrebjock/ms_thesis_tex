\chapter{Data generation details}\label{ch:appendix_data}

\section{SCS and MOSEK convergence times}\label{sec:cvx_times}

Experiments were run in Python using CVX~\citep{cvx} through the Python wrapper CVXPY~\citep{cvxpy}.
Table~\ref{tab:cvx_times_ps} shows the values of $p$ that were used.
We restrained the parameter $p$ for MOSEK as it didn't scale as well as SCS\@.
\begin{table}[!htb]
    \centering
    \setlength{\tabcolsep}{2pt}
    {\small
        \begin{tabular}{|c|c|}\hline
        \textbf{SCS} & \textbf{MOSEK}\\ \hline
        10 & 10\\ \hline
        25 & 25\\ \hline
        50 & 50\\ \hline
        125 & 125\\ \hline
        250 & 150\\ \hline
        500 &\\ \hline
        1000 &\\ \hline
        2000 &\\ \hline
        \end{tabular}
    }%
    \caption[short]{
        Orders of matrices used.
    }
    \label{tab:cvx_times_ps}
\end{table}
For each $p$, 5 replications were performed with matrices of the form $M = D + U \Lambda U^\top$ where $D$ is diagonal,
$\diag D \sim \cU\left[ 0,\, 1 \right]$, $U \in \R^{p \times k}$ for $k = 1,\, 5,\, 10,\, 20,\, 50$
are orthonormal random matrices, and $\Lambda \in \R^{k \times k}$ is diagonal and
$\diag \Lambda^2 \sim \cU\left[ 0,\, 1 \right]$.
No significant difference was measured when changing $k$, even for large values.
The convergence is however much slower if $D$ is scaled up.

\section{Coordinate ascent}\label{sec:coordinate_ascent_data}

The random data is generated using the same scheme presented in Section~\ref{sec:cvx_times} for SCS\@.
As the method scales better, additional sizes $p = 4000,\, 8000$ and $16\,000$ are tested.
We use a tolerance threshold of $10^{-6}$
such that the obtained solution is very close to the one returned by SCS and MOSEK\@.
In practice, very good solutions are attained much faster with a larger tolerance.

\section{Criteo data}\label{sec:criteo_data}

One-hot encoding is not adapted to the Criteo data because there are too many categories.
Another approach would be to one-hot encode only the most frequent categories for each categorical column,
and put the rest in a category \textit{other}.
The winners of the Kaggle competition used the hashing trick.
It consists in choosing an encoding space size $m$,
for example $m = 2^{20}$,
and defining some hash function $h \colon \text{Categories} \to \left\{ 0,\, \dots,\, m - 1 \right\}$.
This approach has several notable advantages.
First, the final encoded feature space $m$ can be adapted depending on the needs and on the computing power.
It may be used in an online fashion without the first pass
that would be required for one-hot encoding in order to figure out all the existing categories.
Lastly, it naturally handles new labels in the test set that were unseen in the train set
(which would typically need a special \textit{other} category in the one-hot encoding setting).
Collisions between categories are likely to happen,
and even collisions between categories from different columns.
We use
$m = 2^{24} = 16\,777\,216$,
and simple hash function,
and only roughly half of the $2^{24}$ hash features were hit by the hash function.
