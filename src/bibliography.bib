@article{sparse_naive_bayes,
	author = {{Askari}, Armin and {d'Aspremont}, Alexandre and {El Ghaoui}, Laurent},
	title = "{Naive Feature Selection: Sparsity in Naive Bayes}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = "2019",
	month = "May",
	eid = {arXiv:1905.09884},
	pages = {arXiv:1905.09884},
	archivePrefix = {arXiv},
	eprint = {1905.09884},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190509884A},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{sparse_center_classifiers,
	author = {{Calafiore}, Giuseppe C. and {Fracastoro}, Giulia},
	title = "{Sparse $\ell_1$ and $\ell_2$ Center Classifiers}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = "2019",
	month = "Nov",
	eid = {arXiv:1911.07320},
	pages = {arXiv:1911.07320},
	archivePrefix = {arXiv},
	eprint = {1911.07320},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191107320C},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{centroid_classification,
	author="Han, Eui-Hong (Sam) and Karypis, George",
	editor="Zighed, Djamel A. and Komorowski, Jan and {\.{Z}}ytkow, Jan",
	title="Centroid-Based Document Classification: Analysis and Experimental Results",
	booktitle="Principles of Data Mining and Knowledge Discovery",
	year="2000",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="424--431",
	abstract="In this paper we present a simple linear-time centroid-based document classification algorithm, that despite its simplicity and robust performance, has not been extensively studied and analyzed. Our experiments show that this centroidbased classifier consistently and substantially outperforms other algorithms such as Naive Bayesian, k-nearest-neighbors, and C4.5, on a wide range of datasets. Our analysis shows that the similarity measure used by the centroid-based scheme allows it to classify a new document based on how closely its behavior matches the behavior of the documents belonging to different classes. This matching allows it to dynamically adjust for classes with different densities and accounts for dependencies between the terms in the different classes.",
	isbn="978-3-540-45372-7"
}

@article{median_centroids,
	author = {Hall, Peter and Titterington, D. and Xue, Jing-Hao},
	year = {2009},
	month = {12},
	pages = {1597-1608},
	title = {Median-Based Classifiers for High-Dimensional Data},
	volume = {104},
	journal = {Journal of the American Statistical Association},
	doi = {10.1198/jasa.2009.tm08107}
}

@article{
	fixed_x_knockoffs,
	author = {{Foygel Barber}, Rina and {Cand{\`e}s}, Emmanuel J.},
	title = "{Controlling the false discovery rate via knockoffs}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	year = "2014",
	month = "Apr",
	eid = {arXiv:1404.5609},
	pages = {arXiv:1404.5609},
	archivePrefix = {arXiv},
	eprint = {1404.5609},
	primaryClass = {stat.ME},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1404.5609F},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{
	model_x_knockoffs,
	author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
	year = {2016},
	month = {10},
	title = {Panning for Gold: Model-free Knockoffs for High-dimensional Controlled Variable Selection},
	volume = {80},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	doi = {10.1111/rssb.12265}
}

@article{bh,
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	month = {11},
	pages = {289 - 300},
	title = {Controlling The False Discovery Rate - A Practical And Powerful Approach To Multiple Testing},
	volume = {57},
	journal = {J. Royal Statist. Soc., Series B},
	doi = {10.2307/2346101}
}

@article{by,
	author = {Benjamini, Yoav and Yekutieli, Daniel},
	year = {2001},
	month = {08},
	title = {The Control of the False Discovery Rate in Multiple Testing Under Dependency},
	volume = {29},
	journal = {Ann. Stat.},
	doi = {10.1214/aos/1013699998}
}

@article{statistical_inference_genome,
	author = {Storey, John and Tibshirani, Robert},
	year = {2003},
	month = {02},
	title = {Statistical Significance for Genome Wide Studies},
	volume = {100},
	journal = {Proc Natl Acad Sci U S A}
}

@article{resampled_fdr_control,
	title = "Resampling-based false discovery rate controlling multiple test procedures for correlated test statistics",
	journal = "Journal of Statistical Planning and Inference",
	volume = "82",
	number = "1",
	pages = "171 - 196",
	year = "1999",
	issn = "0378-3758",
	doi = "https://doi.org/10.1016/S0378-3758(99)00041-5",
	url = "http://www.sciencedirect.com/science/article/pii/S0378375899000415",
	author = "Daniel Yekutieli and Yoav Benjamini",
	keywords = "Model selection, Multiple comparisons, Meteorology, Multiple endpoints",
	abstract = "A new false discovery rate controlling procedure is proposed for multiple hypotheses testing. The procedure makes use of resampling-based p-value adjustment, and is designed to cope with correlated test statistics. Some properties of the proposed procedure are investigated theoretically, and further properties are investigated using a simulation study. According to the results of the simulation study, the new procedure offers false discovery rate control and greater power. The motivation for developing this resampling-based procedure was an actual problem in meteorology, in which almost 2000 hypotheses are tested simultaneously using highly correlated test statistics. When applied to this problem the increase in power was evident. The same procedure can be used in many other large problems of multiple testing, for example multiple endpoints. The procedure is also extended to serve as a general diagnostic tool in model selection."
}

@article{unified_fdr_control,
	author = {Storey, John and Taylor, Jonathan and Siegmund, David},
	year = {2004},
	month = {02},
	pages = {187 - 205},
	title = {Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: A unified approach},
	volume = {66},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	doi = {10.1111/j.1467-9868.2004.00439.x}
}

@inproceedings{lasso,
  title={Regression Shrinkage and Selection via the Lasso},
  author={Robert Tibshirani},
  year={1996}
}

@article{lars,
	author = {{Efron}, Bradley and {Hastie}, Trevor and {Johnstone}, Iain and {Tibshirani}, Robert},
	title = "{Least Angle Regression}",
	journal = {arXiv Mathematics e-prints},
	keywords = {Mathematics - Statistics, 62J07. (Primary)},
	year = "2004",
	month = "Jun",
	eid = {math/0406456},
	pages = {math/0406456},
	archivePrefix = {arXiv},
	eprint = {math/0406456},
	primaryClass = {math.ST},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2004math......6456E},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{lars_complexity,
	author = {Mairal, Julien and Yu, B.},
	year = {2012},
	month = {04},
	title = {Complexity Analysis of the Lasso Regularization Path},
	volume = {1}
}

@inproceedings{odd_ratios,
  title={Feature Selection for Unbalanced Class Distribution and Naive Bayes},
  author={Dunja Mladenic and Marko Grobelnik},
  booktitle={ICML},
  year={1999}
}

@article{interior_point_method_sdp,
	author = {Optimierung, Diskrete and Helmberg, Christoph and Rendl, Franz and Vanderbei, Robert and Wolkowicz, Henry},
	year = {1970},
	month = {02},
	title = {An Interior-Point Method for Semidefinite Programming},
	volume = {6},
	journal = {SIAM Journal on Optimization},
	doi = {10.1137/0806020}
}

@article{sdp_scs,
	author = {O’donoghue, Brendan and Chu, Eric and Parikh, Neal and Boyd, Stephen},
	title = {Conic Optimization via Operator Splitting and Homogeneous Self-Dual Embedding},
	year = {2016},
	issue_date = {June 2016},
	publisher = {Plenum Press},
	address = {USA},
	volume = {169},
	number = {3},
	issn = {0022-3239},
	url = {https://doi.org/10.1007/s10957-016-0892-3},
	doi = {10.1007/s10957-016-0892-3},
	journal = {J. Optim. Theory Appl.},
	month = jun,
	pages = {1042–1068},
	numpages = {27},
	keywords = {Optimization, 49M05, 49M29, Cone programming, 90C06, 90C25, First-order methods, Operator splitting}
}

@article{sdp_admm,
	author="Wen, Zaiwen and Goldfarb, Donald and Yin, Wotao",
	title="Alternating direction augmented Lagrangian methods for semidefinite programming",
	journal="Mathematical Programming Computation",
	year="2010",
	month="Dec",
	day="01",
	volume="2",
	number="3",
	pages="203--230",
	abstract="We present an alternating direction dual augmented Lagrangian method for solving semidefinite programming (SDP) problems in standard form. At each iteration, our basic algorithm minimizes the augmented Lagrangian function for the dual SDP problem sequentially, first with respect to the dual variables corresponding to the linear constraints, and then with respect to the dual slack variables, while in each minimization keeping the other variables fixed, and then finally it updates the Lagrange multipliers (i.e., primal variables). Convergence is proved by using a fixed-point argument. For SDPs with inequality constraints and positivity constraints, our algorithm is extended to separately minimize the dual augmented Lagrangian function over four sets of variables. Numerical results for frequency assignment, maximum stable set and binary integer quadratic programming problems demonstrate that our algorithms are robust and very efficient due to their ability or exploit special structures, such as sparsity and constraint orthogonality in these problems.",
	issn="1867-2957",
	doi="10.1007/s12532-010-0017-1",
	url="https://doi.org/10.1007/s12532-010-0017-1"
}


@book{convex_optimization,
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	title = {Convex Optimization},
	year = {2004},
	isbn = {0521833787},
	publisher = {Cambridge University Press},
	address = {USA}
}

@incollection{markov_blanket,
	title = "Chapter 3 - MARKOV AND BAYESIAN NETWORKS: Two Graphical Representations of Probabilistic Knowledge",
	editor = "Judea Pearl",
	booktitle = "Probabilistic Reasoning in Intelligent Systems",
	publisher = "Morgan Kaufmann",
	address = "San Francisco (CA)",
	pages = "77 - 141",
	year = "1988",
	isbn = "978-0-08-051489-5",
	doi = "https://doi.org/10.1016/B978-0-08-051489-5.50009-6",
	url = "http://www.sciencedirect.com/science/article/pii/B9780080514895500096",
	author = "Judea Pearl"
}

@article{markov_blanket_fs,
	author = {Aliferis, Constantin and Statnikov, Alexander and Tsamardinos, Ioannis and Mani, Subramani and Koutsoukos, Xenofon},
	year = {2010},
	month = {01},
	pages = {171-234},
	title = {Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation},
	volume = {11},
	journal = {Journal of Machine Learning Research},
	doi = {10.1145/1756006.1756013}
}

@article{intro_fs,
	author = {Guyon, Isabelle and Elisseeff, André},
	year = {2003},
	month = {01},
	pages = {1157 - 1182},
	title = {An Introduction of Variable and Feature Selection},
	volume = {3},
	journal = {J. Machine Learning Research Special Issue on Variable and Feature Selection},
	doi = {10.1162/153244303322753616}
}

@article{fs_for_classification,
	title = "Feature selection for classification",
	journal = "Intelligent Data Analysis",
	volume = "1",
	number = "1",
	pages = "131 - 156",
	year = "1997",
	issn = "1088-467X",
	doi = "https://doi.org/10.1016/S1088-467X(97)00008-5",
	url = "http://www.sciencedirect.com/science/article/pii/S1088467X97000085",
	author = "M. Dash and H. Liu",
	keywords = "Feature selection, Classification, Framework",
	abstract = "Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications."
}


@inbook{fs_for_classification_a_review,
	title = "Feature selection for classification: A review",
	abstract = "Nowadays, the growth of the high-throughput technologies has resulted in exponential growth in the harvested data with respect to both dimensionality and sample size. The trend of this growth of the UCI machine learning repository is shown in Figure 2.1. Efficient and effective management of these data becomes increasing challenging. Traditionally, manual management of these datasets has been impractical. Therefore, data mining and machine learning techniques were developed to automatically discover knowledge and recognize patterns from these data.",
	author = "Jiliang Tang and Salem Alelyani and Huan Liu",
	year = "2014",
	month = "1",
	day = "1",
	doi = "10.1201/b17320",
	language = "English (US)",
	isbn = "9781466586741",
	pages = "37--64",
	booktitle = "Data Classification",
	publisher = "CRC Press",
}


@article{original_naive_bayes,
  title={Automatic Indexing: An Experimental Inquiry},
  author={M. E. Maron},
  journal={J. ACM},
  year={1961},
  volume={8},
  pages={404-417}
}

@article{logistic_regression,
	author = {Peng, Joanne and Lee, Kuk and Ingersoll, Gary},
	year = {2002},
	month = {09},
	pages = {3-14},
	title = {An Introduction to Logistic Regression Analysis and Reporting},
	volume = {96},
	journal = {Journal of Educational Research - J EDUC RES},
	doi = {10.1080/00220670209598786}
}

@book{conditional_normal,
	author = {Eaton, Morris L.},
	year = {1983},
	pages = {116–117},
	title = {Multivariate Statistics: a Vector Space Approach},
}

@article{improve_stability_knockoffs,
	author = {{Roquero Gimenez}, Jaime and {Zou}, James},
	title = "{Improving the Stability of the Knockoff Procedure: Multiple Simultaneous Knockoffs and Entropy Maximization}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2018",
	month = "Oct",
	eid = {arXiv:1810.11378},
	pages = {arXiv:1810.11378},
	archivePrefix = {arXiv},
	eprint = {1810.11378},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181011378R},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{schur_complement,
	author = {Z., Zhang},
	year = {2005},
	month = {01},
	title = {The Schur Complement and Its Applications},
	doi = {10.1007/b105056}
}

@article{sklearn,
author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard and Louppe, Gilles},
year = {2012},
month = {01},
title = {Scikit-learn: Machine Learning in Python},
volume = {12},
journal = {Journal of Machine Learning Research}
}

@article{svm,
	author = {Cortes, Corinna and VAPNIK, V.},
	year = {2009},
	month = {01},
	pages = {273-297},
	title = {Support-vector networks},
	volume = {297},
	journal = {Chem. Biol. Drug Des.}
}

@InProceedings{large_movie_review_dataset_acl_2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@article{multi_stage_fdr,
	author = {Tuglus, Catherine and Laan, Mark},
	year = {2009},
	month = {01},
	pages = {Article 12},
	title = {Modified FDR Controlling Procedure for Multi-Stage Analyses},
	volume = {8},
	journal = {Statistical applications in genetics and molecular biology},
	doi = {10.2202/1544-6115.1397}
}

@article{high_dimensional_fs,
	author = {Bermingham, Mairead and Pong-Wong, Ricardo and Spiliopoulou, Athina and Hayward, Caroline and Rudan, Igor and Campbell, H. and Wright, A. and Wilson, J. and Agakov, Felix and Navarro, Pau and Haley, Chris},
	year = {2015},
	month = {05},
	pages = {10312},
	title = {Application of high-dimensional feature selection: Evaluation for genomic prediction in man},
	volume = {5},
	journal = {Scientific Reports},
	doi = {10.1038/srep10312}
}


@article{knn,
	title = "An introduction to kernel and nearest-neighbor nonparametric regression",
	abstract = "Nonparametric regression is a set of techniques for estimating a regression curve without making strong assumptions about the shape of the true regression function. These techniques are therefore useful for building and checking parametric models, as well as for data description. Kernel and nearest-neighbor regression estimators are local versions of univariate location estimators, and so they can readily be introduced to beginning students and consulting clients who are familiar with such summaries as the sample mean and median.",
	author = "Altman, {N. S.}",
	year = "1992",
	month = "8",
	doi = "10.1080/00031305.1992.10475879",
	language = "English (US)",
	volume = "46",
	pages = "175--185",
	journal = "American Statistician",
	issn = "0003-1305",
	publisher = "American Statistical Association",
	number = "3",

}

@article{curse_dimensionality,
	title={A Problem of Dimensionality: A Simple Example},
	author={Gerard V. Trunk},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year={1979},
	volume={PAMI-1},
	pages={306-307}
}

@article{fs_text_classification,
	author = {Forman, George},
	year = {2003},
	month = {03},
	title = {An extensive empirical study of feature selection metrics for text classification [J]},
	volume = {3},
	journal = {Journal of Machine Learning Research - JMLR}
}

@article{gene_selection_cancer_svm,
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	year = {2002},
	month = {01},
	pages = {389-422},
	title = {Gene Selection for Cancer Classification Using Support Vector Machines},
	volume = {46},
	journal = {Machine Learning},
	doi = {10.1023/A:1012487302797}
}

@article{ledoit_wolf,
	title = "A well-conditioned estimator for large-dimensional covariance matrices",
	journal = "Journal of Multivariate Analysis",
	volume = "88",
	number = "2",
	pages = "365 - 411",
	year = "2004",
	issn = "0047-259X",
	doi = "https://doi.org/10.1016/S0047-259X(03)00096-4",
	url = "http://www.sciencedirect.com/science/article/pii/S0047259X03000964",
	author = "Olivier Ledoit and Michael Wolf",
	keywords = "Condition number, Covariance matrix estimation, Empirical Bayes, General asymptotics, Shrinkage",
	abstract = "Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample."
}

@Article{random_forests,
	author="Breiman, Leo",
	title="Random Forests",
	journal="Machine Learning",
	year="2001",
	month="Oct",
	day="01",
	volume="45",
	number="1",
	pages="5--32",
	abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
	issn="1573-0565",
	doi="10.1023/A:1010933404324",
	url="https://doi.org/10.1023/A:1010933404324"
}
