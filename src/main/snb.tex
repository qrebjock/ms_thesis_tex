In this section,
we present a sparse version of naive Bayes elaborated by~\cite{sparse_naive_bayes}.
It allows to perform very fast and accurate feature selection.
We restrict ourselves to the specific case of binary classification, so $\yy \in \bset^n$,
but most key results can naturally be extended to the multiple classes setting.
The negative and the positive classes are noted $\cC_-$ and $\cC_+$ respectively,
and will be indifferently substituted with their respective labels.

\subsection{Reminders on vanilla naive Bayes}\label{subsec:naive_bayes}

Naive Bayes was first introduced by~\cite{original_naive_bayes} for text documents classification.
It is a simple model assuming the independence of the features, given the label.
Despite this simple presupposition (that is very likely to fail in practice),
naive Bayes remains an engaging model for large scale datasets because of its low complexity.
The time complexity to train a model is asymptotically $\cO\left( n \cdot p \right)$,
where $n$ and $p$ are the number of samples and the number of features respectively.
Another appealing propriety is that naive Bayes can be trained in an online fashion,
as new data points come in a sequential order.
This aspect can be particularly helpful if the dataset doesn't fit in memory.
Furthermore, distributed implementations could even be considered in order to speed up the learning process.

\paragraph{General settings.}\label{subsubsec:nb_general}

Note $\left( \cX,\, \cY \right)$ the pair of random variables that generated the samples and targets $X$ and $\yy$.
The goal is to explain $\yy$ given $X$, that is, finding the posterior probabilities $\Pr(\cC_\pm \mid \cX)$.
From these probabilities, a new observation $\xx \in \R^p$ is classified according to the highest
posterior probability between the two classes
\begin{equation}\label{eq:nb_inference}
y\left( \xx \right) = \underset{\cC \in \bclasses}{\argmax} \Pr\left( \cY = \cC \mid \cX = \xx \right)
\end{equation}
To do so, we combine the use of Bayes rule and make the (very) naive assumption that
all the features are independent given the class, that is
\begin{equation}
    \Pr\left( \cX = \xx \mid \cC_\pm \right) = \prod_{j = 1}^p \Pr(\cX_j = x_j \mid \cC_\pm)
    ,\qquad
    \Pr(\cC_\pm \mid \cX = \xx) = \frac{\Pr(\cX = \xx \mid \cC_\pm) \cdot \Pr(\cC_\pm)}{\Pr(\cX = \xx)}
\end{equation}
On the right hand side,
the denominator $\Pr(\cX = \xx)$ does not depend on the class $\cC_\pm$.
It is therefore not required to evaluate it in order to perform the inference described in~\ref{eq:nb_inference}.
Thus, we only need to estimate the probabilities $\Pr(\cC_\pm)$ and $\Pr(\xx \mid \cC_\pm)$.
The former are simply data averages, i.e.\ the frequencies of the positive and negative classes in the observed data.
As for the latter probabilities
$\Pr\left( \cX = \xx \mid \cC_\pm \right) = \prod_{j = 1}^p \Pr(\cX_j = x_j \mid \cC_\pm)$,
they can be modeled by a plethora of distribution families, depending on the prior knowledge we have on the data.
The distribution is typically parametrized by some vector $\btheta \in \R^m$.
which allows the probabilities to be computed by maximizing the likelihood $\cL$ of the observed data.
Equivalently we may also maximize the log-likelihood $\lglh = \log\cL$.
\begin{equation*}
    \lglh\left( \btheta \right) = \sum_{i = 1}^n \Pr\left( \cX = \xx_i \mid y_i \,; \btheta \right)
\end{equation*}
We present now 3 meaningful cases, where the prior distributions of $\cX \mid \cC_\pm$ are either
gaussian, bernoulli or multinomial.
Let $\cI_\pm = \big\{ i \in \pset \mid y_i = \cC_\pm \big\}$ be the sets containing the
indexes of the positive and negative data points.
We also note for each class $\cC_\pm$ its cardinality and empirical sum respectively, as follows:
\begin{equation*}
    n^\pm = | \cI_\pm |,
    \qquad\qquad
    \ff^\pm = \sum_{i \in \cI_\pm} \xx_i
\end{equation*}

\paragraph{Gaussian naive Bayes}\label{subsubsec:gnb}

In this case the observed data conditioned on its label is modeled by a gaussian distribution
$\cN( \bmu^\pm,\, \Sigma^\pm )$.
It is the most common configuration because it can account for continuous data and the normal distribution constitutes
a decent prior in many cases by virtue of its high entropy.
Note that the covariance $\Sigma^\pm \in \R^{p \times p}$
is diagonal as a result of the independence assumption that we made.
\begin{equation*}
    \Pr\left( \cX = \xx \mid C_\pm \right) =
    \frac{1}{\sqrt{(2\pi)^p\det(\Sigma_\pm)}}
    \exp\Big( -\frac{1}{2}(\xx - \bmu_\pm)^\top\Sigma^{-1}(\xx - \bmu_\pm) \Big)
\end{equation*}
By denoting $\sigma_j = \Sigma_{j j}$, the log-likelihood can be written
\begin{equation*}
    \begin{split}
        \lglh_g\left( \bmu_+,\, \ssigma_+,\; \bmu_-,\, \ssigma_-\right ) &=
        \sum_{j = 1}^p \Bigg[
            \sum_{i \in \cI_+}
                -\frac{1}{2} \log \left( 2\pi \right)
                -\log \sigma_j^+
                -\frac{(x_j - \mu^+_j)^2}{2{\sigma^+_j}^2}\\
            &\qquad\quad+ \sum_{i \in \cI_-}
                -\frac{1}{2} \log \left( 2\pi \right)
                -\log \sigma_j^-
                -\frac{(x_j - \mu^-_j)^2}{2{\sigma^-_j}^2}
        \Bigg]
    \end{split}
\end{equation*}
Even if $\lglh_g$ is not concave, it maximizer admits a closed-form solution
(readily shown to be the point where the gradient is null)
\begin{align*}
    \bmu^\pm = \frac{\ff^\pm}{n^\pm}
    ,\qquad\quad
    \ssigma^\pm = \sqrt{\frac{1}{n^\pm} \sum_{i \in \cI^\pm} (\xx_i - \bmu^\pm)^2}
\end{align*}

\paragraph{Bernoulli naive Bayes}\label{subsubsec:bnb}

The Bernoulli distribution assumes that the design matrix is binary,
that is $X \in \zoset^{n \times p}$.
Even though this situation isn't very prevalent in practice, it has a simple and elegant solution.
More formally, we assume the existence of
$\btheta^+,\, \btheta^- \in \left( 0,\, 1 \right)^p$
such that for any data point $\xx \in \R^p$ the conditional probabilities are given by
\begin{equation*}
    \Pr\left( \cX_j = x_j \mid \cC_\pm \right) = (\theta^\pm_j)^{x_j} \cdot (1 - \theta^\pm_j)^{1 - x_j}
\end{equation*}
It yields that $\log \Pr\left( \cX = \xx \mid \cC_\pm \right) =
\xx^\top\log\btheta^\pm + (\1 - \xx)^\top\log\left( \1 - \btheta^\pm \right)$
and finally
\begin{align}\label{eq:bernoulli_snb_ll}
    \lglh_b\left( \btheta^+,\, \btheta^- \right) &=
        \ff_+^\top\log\btheta^+ + (n_+\1 - \ff_+)^\top\log\left( \1 - \btheta^+ \right)\nonumber\\
        &\qquad+ \ff_-^\top\log\btheta^- + (n_-\1 - \ff_-)^\top\log\left( \1 - \btheta^- \right)
\end{align}
The independence assumption makes the optimization problem decomposable across features;
it reduces to $p$ simpler maximizations, each of them being concave and admitting a closed-form solution.
Finally, we find that
\begin{equation*}
    \theta_\pm^\star = \frac{f^\pm}{n^\pm}
    ,\qquad\qquad
    \text{which are simply the averages of each class.}
\end{equation*}

\paragraph{Multinomial naive Bayes}\label{subsubsec:mnb}

Multinomial naive Bayes generalizes the Bernoulli version
as it supposes that $X \in \N^{n \times p}$ is generated by the following underlying distribution
\begin{equation}\label{eq:multinomial_pr}
\Pr\left( \cX = \xx \mid \cC_\pm \right) =
\frac{\big( \sum_{j = 1}^p x_j \big)!}
{\prod_{j = 1}^p x_j!} \cdot \prod_{j = 1}^p (\theta^\pm_j)^{x_j}
\end{equation}
In the special case where $X \in \zoset^{n \times p}$ we recover the above Bernoulli formulation.
It is parametrized by $\btheta^+,\, \btheta^- \in \left( 0,\, 1 \right)^p$, and they must satisfy
$\1^\top\btheta^+ = \1^\top\btheta^- = 1$ for~\ref{eq:multinomial_pr} to be a proper distribution.
Note that this model is still valid in the more general case
where we only assume the data to be non-negative, $X \in \R_+^{n \times p}$.
Hence, it is not as restrictive as it may appear at first sight,
as it is applicable to a large number of datasets.
The log probability is given by
\begin{equation*}
    \log\Pr\left( \cX = \xx \mid \cC_\pm \right) =
    \xx^\top\log\btheta_\pm + \log\frac{\big(\sum_{j = 1}^p x_j\big)!}{\prod_{j = 1}^p x_j!}
\end{equation*}
and the log-likelihood reduces to (after removing the constant terms)
\begin{equation}\label{eq:multinomial_snb_ll}
\lglh_m\left( \btheta^+,\, \btheta^- \right) = \ff_+^\top\log\btheta^+ + \ff_-^\top\log\btheta^-
\end{equation}
which is again decomposable across features.
It turns out that the optimums are
\begin{equation*}
    \btheta_\pm^\star = \frac{\ff^\pm}{\1^\top\ff^\pm}
\end{equation*}
\bigbreak
In the models presented above, the time complexity to train the naive Bayes classifier is $\cO(n \cdot p)$,
and the solutions can be computed in closed-form, which makes the effective computation cost very low.
In comparison, no closed-form solution exist for the Lasso~\citep{lasso},
for logistic regression~\citep{logistic_regression}, and for SVMs~\citep{svm}.

\paragraph{Decision boundary}\label{subsubsec:nb_bound}

Given a new data point $\xx \in \R^p$, we wish to attribute it the most probable label
$y \in \left\{ \cC^-,\, \cC^+ \right\}$.
No matter what model parametrized by $\btheta$ was chosen,~\ref{eq:nb_inference} reduces to
\begin{align*}
    y &= \underset{\cC \in \bclasses}{\argmax} \Pr\left( \cC \mid \xx \right)\\
    &= \sign\bigg[\log \frac{\Pr(\cC^+)}{\Pr(\cC^-)}
        + \log \frac{\Pr(\xx \mid \cC^+)}{\Pr(\xx \mid \cC^-)}\bigg]
\end{align*}
In the cases of \emph{bernoulli} (\ref{subsubsec:bnb}) and \emph{multinomial} (\ref{subsubsec:mnb}) naive Bayes,
there exist $v \in \R$ and $\ww \in \R^p$ such that $y = \sign(v + \ww^\top\xx)$.
In these two special cases, the decision boundary is linear (which doesn't happen for gaussian naive Bayes).
For both of them $v$ has the same value, and by noting $\ww_b$ and $\ww_m$ the weights for the bernoulli
and the multinomial case respectively, we have
\begin{equation*}
    v = \log\frac{\Pr(\cC^+)}{\Pr(\cC^-)}
    ,\qquad\quad
    \begin{cases*}
        \ww_b = \log(\btheta^+\odot(\1 - \btheta^-)) - \log(\btheta^-\odot(\1 - \btheta^+))\\
        \ww_m = \log\btheta^+ - \log\btheta^-
    \end{cases*}
\end{equation*}

\subsection{Sparse naive Bayes (SNB)}\label{subsec:snb}

By adding a sparsity constraint in the Bernoulli
and the multinomial optimization problems portrayed above~\cite{sparse_naive_bayes}
introduced a version of naive Bayes that imposes the weight vector $\ww$
to have a number of non-zero entries under a certain threshold $k \in \N$.
This property makes naive Bayes employable to perform feature selection,
by keeping only the features whose weights are non-zero.

\paragraph{Problem statement.}\label{subsubsec:snb_ps}

Let $0 \leq k \leq p$ be a desired level of sparsity.
We wish to train a naive Bayes classifier whose decision boundary depends on at most $k$ features.
As shown in the previous section~\ref{subsubsec:nb_bound},
for both bernoulli and multinomial naive Bayes
there exist $v \in \R$ and $\ww \in \R^p$ such that $y(\xx) = \sign(v + \ww^\top\xx)$ for any data point $\xx\in\R^p$.
Furthermore, the $j$th entry of the decision vector $\ww$ is null if and only if $\btheta^-_j = \btheta^+_j$,
where $\btheta^-$ and $\btheta^+$ are the parameters of the loss functions $\lglh_b$ and $\lglh_m$.
Naturally, imposing the constraint $\norm{\btheta^+ - \btheta^-}_0 \leq k$ in the optimization problems
will yield a weight vector $\ww$ with the desired sparsity.
The optimization problems for Bernoulli and multinomial SNB
(shortened BSNB and MSNB respectively) can be phrased as follows:

\begin{equation}\label{eq:bsnb}\tag{BSNB}
\begin{aligned}
    & \underset{\btheta^+,\, \btheta^-}{\text{maximize}}
    \quad&&\lglh_\text{b}\left( \btheta^+,\, \btheta^- \right)
    = \ff_+^\top\log\btheta^+ + (n_+\1 - \ff_+)^\top\log\left( \1 - \btheta^+ \right)\\
    & && \qquad\qquad\qquad\qquad + \ff_-^\top\log\btheta^- + (n_-\1 - \ff_-)^\top\log\left( \1 - \btheta^- \right)\\
    & \text{subject to}
    &&\norm{\btheta^+ - \btheta^-}_0 \leq k.
\end{aligned}
\end{equation}
\vspace{0.25cm}
\begin{equation}\label{eq:msnb}\tag{MSNB}
\begin{aligned}
    & \underset{\btheta^+,\, \btheta^-}{\text{maximize}}
    & & \lglh_\text{m}\left( \btheta^+,\, \btheta^- \right) = \ff_+^\top\log\btheta^+ + \ff_-^\top\log\btheta^-\\
    & \text{subject to}
    & & \norm{\btheta^+ - \btheta^-}_0 \leq k
    \quad\text{and}\quad \1^\top\btheta^+ = \1^\top\btheta^- = 1.
\end{aligned}
\end{equation}

\paragraph{Main results and resolution.}\label{subsubsec:snb_th}

Surprisingly, despite the combinatorial constraints
these optimizations problems can be (approximately) solved very efficiently,
with an additional minor cost compared to vanilla naive Bayes.
Especially for the Bernoulli case,
an optimal solution can be computed in closed-form as shown in Theorem~\ref{th:bsnb}.
\begin{theorem}\label{th:bsnb}
Suppose that $X \in \left\{ 0, 1 \right\}^{n \times p}$ is modeled by the Bernoulli distribution.
Then, the exact solution to the problem~\ref{eq:bsnb} can be computed.
First, define $\mt$ and $\uu$ as follows
\begin{align*}
    \mt &= (\ff^+ + \ff^-) \odot \log\left( \frac{\ff^+ + \ff^-}{n} \right)
    + (n\1 - \ff^+ - \ff^-) \odot \log\left( \1 - \frac{\ff^+ + \ff^-}{n} \right)\\
    \begin{split}
        \uu &= \ff^+ \odot \log \frac{\ff^+}{n^+} + (n^+\1 - \ff^+) \odot \log (\1 - \frac{\ff^+}{n^+})
        + \ff^- \odot \log \frac{\ff^-}{n^-} + (n^-\1 - \ff^-) \odot \log (\1 - \frac{\ff^-}{n^-})
    \end{split}
\end{align*}
Let $\cI$ be the set of $p - k$ smallest elements of $\uu - \mt$.
The optimal $\btheta^-,\, \btheta^+$ are given by
\begin{equation*}
    {\theta^-_j}^\star = {\theta^+_j}^\star = \frac{1}{n}(f_j^+ + f_j^-)
    \;\forall j \in \cI
    ,\qquad
    {\theta^\pm_j}^\star = \frac{f^\pm_j}{n^\pm}
    \;\forall j \notin \cI
\end{equation*}
\end{theorem}
In total, the maximizer can be found in $\cO\left( n \cdot p + p \cdot \log k \right)$ steps,
which is very close to the cost $\cO\left( n \cdot p \right)$ of naive Bayes.

In the multinomial case, there is no closed-form solution, but a near-optimal one can be obtained as
stated in Theorem~\ref{th:msnb}.
\begin{theorem}\label{th:msnb}
    Suppose that $X \in \R_+^{n \times p}$ is generated by the multinomial distribution.
    Define $\phi_k : \alpha \mapsto s_k(\hh(\alpha)) + C$ where $C$ is some constant,
    $s_k$ the sum of the k largest values of a vector, and
    \begin{equation*}
            \hh(\alpha) = \ff_+ \odot \log \ff_+ + \ff_- \odot \log \ff_-
                - (\ff_+ + \ff_-) \odot \log (\ff_+ + \ff_-)
                - \ff_+ \log \alpha - \ff_- \log (1 - \alpha)
    \end{equation*}
    $\phi_k$ is the dual of~\ref{eq:msnb} and can be minimized very quickly (with bisection for example)
    as it is a one-dimensional convex function.
    Let $\alpha^\star$ be its minimizer, $\cI$ the set of the $p - k$ smallest entries of
    $\hh(\alpha^\star)$, and $B_\pm = \sum_{j \notin \cI} f_i^\pm$.
    A primal point can be reconstructed as follows:
    \begin{equation*}
        {\theta^+_j}^\star = {\theta^-_j}^\star = \frac{f_j^+ + f_j^-}{\1^\top(\ff^+ + \ff^-)}
        \;\forall j \in \cI
        ,\qquad
        {\theta^\pm_j}^\star = \frac{B_+ + B_-}{B_\pm}\frac{f^\pm_j}{\1^\top(\ff^+ + \ff^-)}
        \;\forall j \notin \cI
    \end{equation*}
    Furthermore, it holds that $\psi(k - 4) \leq \phi(k) \leq \psi(k) \leq \phi(k + 4)$,
    implying that the duality gap is small if $\psi(k) - \psi(k - 4)$ is small.
\end{theorem}
Experimentally, the duality gap quickly converges to $0$ as $k$ increases,
and the reconstructed primal point is near-optimal.
The time complexity is once again $\cO(n \cdot p + p \cdot \log k)$,
which is a minor additional cost compared to plain naive Bayes.

The authors experiment SNB on several text datasets.
They obtain competitive test accuracies,
while training their models several order of magnitude faster than the Lasso.
For these reasons, SNB appears as an attractive alternative to the Lasso for the computation of statistics
in the knockoff procedure~\ref{sec:ksc}.
