\chapter{The knockoffs filter framework}\label{ch:knockoffs}

Recently~\cite{fixed_x_knockoffs} introduced the knockoffs framework to perform feature selection.
The goal is to control the false discovery rate as defined in Section~\ref{sec:fdrc},
that is, maintaining it under some threshold
(while maximizing the power as much as possible).
More formally, let $X \in \R^{n \times p}$ be a feature matrix and $\yy \in \R^n$ the associated target vector.
Suppose $\yy$ depends only on the subset of features $\cS \subseteq \fset$.
Given some $\fdr$ target $\fdrtarget \in [0, 1]$, we wish to find a procedure such as, in average,
the false discovery proportion is smaller that $\fdrtarget$, i.e. $\fdr \leq q$.
To do so, the key idea is to construct for each original feature $X_j$, $j \in \fset$,
a knockoff (that is to say, fake) feature $\tX_j$ which is known to be out of the model.
Original features are then selected only if they prove to be more significant than their knockoff counterparts.

To compare a feature and its knockoff, several quantities will be computed by interchanging matrix columns.
For this reason, we define the $\swap$ operator.
\begin{definition}\label{def:swap}
    \text{($\swap$ operator)}
    Given two matrices $A,\,B \in \R^{n \times p}$ with the same size,
    we define the $\swap$ operator on the concatenated matrix $\big[ A,\, B \big]$ as follows.
    For a any subset of indices $S \subseteq \left\{ 1, \dots, p \right\}$,
    $\big[ A,\, B \big]_{\swap(S)}$ is the transformed matrix where the columns $A_j$ and $B_j$ were swapped for all
    $j \in S$.
\end{definition}

In the following sections, we are going to detail principal aspects of the construction of the knockoff features,
the computation of statistics for both original and knockoff features,
and the feature selection itself, based on those statistics.

\section{Knockoffs construction}\label{sec:kc}

In this section, we focus on the construction of the knockoff matrix.
At first~\cite{fixed_x_knockoffs} introduced what they called the
\textit{fixed}-X knockoffs variable selection procedure.
It relies on the creation of fake features satisfying some correlation constraints with the original features.
Unfortunately, it can only perform adequately when $n \geq 2p$,
even though it can be partially adapted to the case where $n \geq p$.
Later~\cite{model_x_knockoffs} proposed an extension of the framework called \textit{model}-X knockoffs,
in which knockoff features are sampled from a learned distribution.
Despite the restriction of \textit{fixed}-X knockoffs, this method is still appealing as the construction of the
knockoff variables is straightforward (and moderately costly).
On the other hand, \textit{model}-X knockoffs work in higher dimensions
but need an estimation of the distribution that generated the data,
which is a hard problem in general.
We will focus on the construction of \textit{model}-X knockoffs as we are principally interested in the
high dimensional setting.

In this section, let $(\cX,\, \cY)$ be a pair of random variables,
$\cX$ being composed of $p$ features,
and $\cY$ the associated target in $\R$.
We suppose that the feature matrix and the target vector $(X,\, \yy)$
are composed of independent samples from $(\cX,\, \cY)$.

\subsection{General case}\label{subsec:general_case}

We wish to build knockoff features $\tX \in \R^{n \times p}$,
and to do so we are going to sample from a
random variable $\tcX$ built such that $\big[ \cX; \tcX \big]$
follows the properties~\ref{itm:cond_swap} and~\ref{itm:cond_indep} defined thereafter.
\begin{definition}
    \text{(model-$X$ knockoffs)}
    Given random vector $\cX \in \R^p$ of features,
    a random vector $\tcX \in \R^p$ is said to be model-$\cX$ knockoffs with respect to $\cY$
    if it satisfies the two following properties:
    \begin{enumerate}[label=\textbf{S.\arabic*},ref=S.\arabic*]
        \item \label{itm:cond_swap} For any $S \subseteq \fset$,
            $\big[ \cX; \tcX \big]^\top_{\swap(S)} \distreq \big[ \cX; \tcX \big]^\top$
        \item \label{itm:cond_indep} $\tcX \independent \cY \mid \cX$
    \end{enumerate}
\end{definition}
Intuitively, the condition~\ref{itm:cond_swap} ensures that a knockoff feature is sufficiently
close to its associated original feature so that swapping them doesn't change the distribution of the
concatenated random vector.
The independence condition~\ref{itm:cond_indep} states that the knockoff features
carry no additional information on $\cY$, given $\cX$.
It is trivially satisfied if $\tX$ is built without exploiting $\yy$.
However, constructing knockoffs meeting the first distribution equality is practically infeasible in general.
\begin{remark}
    By comparison, constructing \textit{fixed}-X knockoffs $\tX \in \R^{n \times p}$ is a deterministic process.
    Knockoff features must satisfy the two following covariance equalities.
    \begin{equation*}
        \begin{cases*}
            \tX^\top \tX = \Sigma\\
            X^\top \tX = \Sigma - \diag\bs
                \quad\text{where } \bs \text{ is a non negative vector.}
        \end{cases*}
    \end{equation*}
    It ensures that $\tX$ has the same covariance as the original matrix $X$
    and that the correlation between distinct original and knockoff variables is
    the same as the correlation between the two originals.

    A solution to these two equations is guaranteed to exist only if $2p \leq n$.
    Moreover, fewer test statistics yield theoretical guarantees regarding the FDR control
    of the procedure,
    and \textit{model}-X knockoffs tend to give a higher statistical power experimentally.
    In the coming sections,
    we will only consider \textit{model}-X knockoffs as we are interested in the high dimensional setting.
\end{remark}

\subsection{Gaussian case}\label{subsec:gaussian_knockoffs}

In the particular case where $\cX$ is multivariate gaussian, the exact distribution of $\tcX$ can be derived.
\begin{proposition}
    \label{prop:gaussian_knockoffs}
    Suppose that $\cX \sim \cN(\bmu,\, \Sigma)$.
    If $\tcX$ is a random variable such that
    \begin{equation*}
        \begin{bmatrix} \cX\\ \tcX \end{bmatrix} \sim \cN\bigg(\begin{bmatrix} \bmu\\ \bmu \end{bmatrix},\, \Omega\bigg)
        ,\quad\text{where }\quad
        \Omega = \begin{bmatrix}
            \Sigma & \Sigma - \diag \bs\\
            \Sigma - \diag \bs & \Sigma
        \end{bmatrix}
        \quad\text{for some }
        \bs \in \R^p,
    \end{equation*}
    then $\big[\cX; \tcX \big]$ satisfies the $\swap$ property~\ref{itm:cond_swap},
    provided that $\Omega$ is positive semidefinite (so that it is indeed a covariance matrix).
\end{proposition}
In Proposition~\ref{prop:gaussian_knockoffs},
$\bs \in \R^p$ can be any vector such that $\Omega \succeq \0$.
We will come back later to the choice of $\bs$ which is actually crucial.
For now, assume that $\bs$ satisfies this assumption.
This result gives a way of constructing the knockoff features from $X$.
Indeed, as $\big[ \cX; \tcX \big]$ is multivariate normal, we may compute the exact distribution of
$\tcX \mid \cX$ with classical conditional formulas~\citep{conditional_normal}
as shown in~\ref{eq:conditional_gaussian_knockoffs}.
\begin{equation}\label{eq:conditional_gaussian_knockoffs}
    \tcX \mid \cX \sim \cN\left( \bupsilon,\, \Upsilon \right)
    ,\qquad\text{where}\quad
    \begin{cases*}
        \bupsilon = \cX - \cX\Sigma^{-1}\diag\{ \bs \}\\
        \Upsilon = \diag\{ \bs \} \left( 2I_{p \times p} - \Sigma^{-1}\diag\{ \bs \} \right)
    \end{cases*}
\end{equation}
To put this into practice, one would compute the empirical mean
$\hat{\bmu} \in \R^p$ and covariance $\hat{\Sigma} \in \R^{p \times p}$ of $\cX$
using the observed feature matrix $X$.
Then, each row of $\tX$ is sampled according to a gaussian distribution $\cN\left( \bupsilon,\, \Upsilon \right)$
whose parameters are described in~\ref{eq:conditional_gaussian_knockoffs}.
Note in particular that the construction process is random;
if it is repeated, it may very well return different knockoffs, and thus different
selected features in the end.
Because of this instability,
several people attempted to aggregate knockoffs
and reduce the variance in the selection~\citep{improve_stability_knockoffs, aggregation_knockoffs, aggregated_fdr}.
Depending on the available computing power,
various algorithms may be used to estimate the covariance matrix $\Sigma$.
The empirical estimator is cheap but known to be imprecise when $p > n$.
Shrunk estimators like the one proposed by~\cite{ledoit_wolf} provide better results in high dimension.

\bigbreak
The gaussian hypothesis is obviously rarely verified in practice but yields acceptable results even when
$\cX$ is far from gaussian.
It partly comes from the fact that, rather than constructing $\tcX$ to respect~\ref{itm:cond_swap},
a weaker condition would be to enforce $\big[ \cX; \tcX \big]$ and $\big[ \cX; \tcX \big]_{\swap(S)}$ to have the
same first two moments (mean and covariance).
It turns out to be the case if $\tcX \mid \cX$ is constructed as described in~\ref{eq:conditional_gaussian_knockoffs}.
In the remaining of this master thesis, we will restrain ourselves to the gaussian hypothesis.
Note that generating knockoffs requires an estimation of the distribution of $\cX$ only,
and not $\cY \mid \cX$ as most methods would.
It is particularly appealing because labeling data is often the most costly part,
while acquiring samples $\xx \sim \cX$ is easier.

\section{Statistics computation}\label{sec:ksc}

We suppose now that the distribution of $\cX$ was determined and that we are able to sample from it.
It gives a knockoff matrix $\tX \in \R^{n \times p}$ of the same size as $X$.

\subsection{General principle}\label{subsec:scg}

Given the original feature matrix and the sampled knockoffs $X,\, \tX \in \R^{n \times p}$,
statistics $w_j$ are computed for all $j \in \fset$.
Each $w_j$ represents how more significant the original feature $X_j$ is compared to $\tX_j$.
These statistics must satisfy the \emph{flip-sign} technical condition~\ref{def:flip_sign}
for the FDR control to carry out,
but a wide variety of choices is possible as will be shown.
\begin{definition}\label{def:flip_sign}
\text{(flip-sign property)}
A statistics function $\omega \colon \R^{n \times 2p} \times \R^n \to \R^p$
is said to follow the flip-sign property if for any $S \subseteq \fset$ and any $j \in \fset$,
\begin{equation*}
    \omega_j\big( \big[ X,\, \tX \big]_{\swap(S)},\, \yy \big) = \begin{cases*}
        -\omega_j\big( \big[ X,\, \tX \big],\, \yy \big) &\quad\text{if $j \in S$.}\\
        \omega_j\big( \big[ X,\, \tX \big],\, \yy \big) &\quad\text{otherwise.}
    \end{cases*}
\end{equation*}
\end{definition}
This property is very natural,
as it is simply asking the original and the knockoff features to play antisymmetric roles.

\subsection{Statistics aggregation}\label{subsec:ksa}

Constructing statistics satisfying the \emph{flip-sign} property~\ref{def:flip_sign} is actually straightforward
as an elementary scheme in two steps leads to such statistics.
The idea is to build statistics for each original and each knockoff feature, and then aggregate them.
\begin{enumerate}
    \item First construct statistics $[ \zz; \tilde{\zz} ] = \zeta\big( \big[ X,\, \tX \big],\, \yy \big)$
        with some function $\zeta \colon \R^{n \times 2p} \times \R^n \to \R^{2p}$ satisfying
        \begin{equation}\label{eq:swap_cond}
        [ \zz; \tilde{\zz} ]_{\swap(S)} = \zeta\big( \big[ X,\, \tX \big]_{\swap(S)},\, \yy \big)
        ,\quad
        \forall S \subseteq \fset
        \end{equation}
        The statistics $z_j$ (resp. $\tilde{z}_j$) indicates how significant the original (resp.\ knockoff) feature $j$ is.
    \item Then aggregate for each $j \in \fset$ the statistics of the original feature $z_j$ and the one of the corresponding
        knockoff $\tilde{z}_j$ with an antisymmetric function $a_j \colon \R\times\R \to \R$.
        That is, set $w_j = a_j(z_j,\, \tilde{z}_j)$.
\end{enumerate}
It is easy to show that such constructed statistics will satisfy the \emph{flip-sign} property~\ref{def:flip_sign}.

Basically any antisymmetric function $a_j$ could work, but some choices lead to a better power experimentally.
Here are a few examples of mappings:
\begin{itemize}
    \item $w_j = z_j - \tilde{z}_j$ (experimentally gives highest power in many cases)
    \item $w_j = \max(z_j, \tilde{z}_j) \cdot \sign(z_j - \tilde{z}_j)$ (first proposed by~\cite{fixed_x_knockoffs})
    \item $w_j = \log\frac{z_j}{\tilde{z}_j}$
\end{itemize}
As for the function $\zeta$, it only needs to satisfy the \emph{swap} property~\ref{eq:swap_cond}.
This condition may seem restrictive but a large number of choices are actually valid.

\subsection{Examples}\label{subsec:sce}

We illustrate here a few examples of valid mappings $\zeta$
to build aggregated knockoff statistics.
After empirical observations~\cite{fixed_x_knockoffs}
suggest to use the Lasso Signed Max (LSM) statistics defined as follows.
\begin{equation}
    z_j = \sup\{\lambda \mid \hat{\bbeta}_j(\lambda) \neq 0\}
    ,\qquad\quad
    \hat{\bbeta}(\lambda) =
    \underset{\bb}{\argmin}\;
    \frac{1}{2}\big\lVert \yy - \big[ X,\, \tX \big]\bb \big\rVert_2^2 + \lambda\norm{\bb}_1
\end{equation}
The vector $\hat{\bbeta}(\lambda) \in \R^{2p}$ contains the coefficients of a Lasso model
with penalty coefficient $\lambda > 0$.
As mentioned in Subsection~\ref{subsubsec:lasso},
all the coefficients are null starting from $\lambda = +\infty$,
and are likely to shift (and to grow in absolute value) as $\lambda \to 0$.
It makes sense to use the first point where the coefficient $\beta_j$ becomes non-null
as a significance metric for each individual feature $j$.
In addition, the algorithm LARS is able to compute that value pretty efficiently~\citep{lars_complexity}.

Another possibility proposed by~\cite{model_x_knockoffs} is to train a plain Lasso estimator
and keep its coefficient in absolute value.
Here again, the LARS algorithm allows to cross-validate the penalty $\lambda$ efficiently.
Experimentally, these statistics often give remarkable results in terms of power.

More generally, the coefficients in absolute value of any reasonable regressor
(or classifier, depending on the task) constitute a judicious option of statistics.
The fact that the \emph{flip-sign} property isn't restrictive makes the knockoff framework very robust.
An adapted model can be trained depending on the data, be it logistic regression,
SVMs, or random forests~\cite{random_forests}.
When the model has hyper-parameters, those are typically tuned using cross-validation.

\section{Selection thresholds}\label{sec:kfs}

The selection itself requires the constitution of a data-dependent threshold $\tau$
conditioned by the target FDR $\fdrtarget \in \zoint$.
In the end, only the features $j$ whose statistics $w_j$ are above the threshold will be selected.
\begin{equation}
    \hat{\cS} = \left\{ j \mid w_j \geq \tau \right\}
\end{equation}
The threshold $\tau$ can be adapted depending on how restrictive the procedure ought to be.
It leads to different guarantees regarding the control of the FDR\@.
Two selection procedures are established by~\cite{fixed_x_knockoffs},
namely \emph{knockoff} and \emph{knockoff+}.
They control the modified FDR (as defined below) and the FDR respectively.
\begin{definition}\label{def:mfdr}
Given an estimate $\hat{\cS}$ of $\cS$ and a desired false discovery rate target $q \in [0, 1]$,
the modified $\fdr$ is defined as follows:
\begin{equation*}
    \mfdr = \E{
        \frac
            {\big\lvert \hat{\cS} \setminus \cS \big\rvert}
            {\lvert\hat{\cS}\rvert + 1 / q}
    }
\end{equation*}
\end{definition}
Since $0 \leq q \leq 1$, $\mfdr$ as described in~\ref{def:mfdr} is always smaller than the actual $\fdr$.
It means that controlling the $\mfdr$ is less restrictive than controlling the $\fdr$,
and the $\fdr$ could potentially be much larger than the $\mfdr$.
But if the target threshold $q$ is not too small, and if many features are selected by the procedure,
the modified version of the FDR is close enough to the actual $\fdr$.
Being less restrictive by controlling only the $\mfdr$ can greatly improve the power
of the selection.

The only difference between the \emph{knockoff} and \emph{knockoff+} procedures is the selection threshold $\tau$.
\begin{definition}
    (knockoff and knockoff+ thresholds)
    Given the statistics $\ww \in \R^p$ computed from $X$ and $\tX$,
    we define the knockoff and knockoff+ thresholds respectively as follows:
    \begin{align}
        \tau &=
        \min\left\{
            t > 0 \mid \frac
                { \#\left\{ j \mid w_j \leq -t \right\} }
                { \#\left\{ j \mid w_j \geq t \right\} }
            \leq q
        \right\}\label{eq:knock_thres}\\[7pt]
        \tau^+ &=
        \min\left\{
            t > 0 \mid \frac
                { 1 + \#\left\{ j \mid w_j \leq -t \right\} }
                { \#\left\{ j \mid w_j \geq t \right\} }
            \leq q
        \right\}\label{eq:knockp_thres}
    \end{align}
\end{definition}
These thresholds only differ by their numerator, where $\tau^+$ has an additional $+1$.
The main result of~\cite{fixed_x_knockoffs, model_x_knockoffs}
regarding the control of the $\mfdr$ and $\fdr$ is stated in Theorem~\ref{th:knockoffs}.
\begin{theorem}\label{th:knockoffs}
    (guarantees of the knockoff procedures)
    Construct $\hat{\cS} = \left\{ j \mid w_j \geq \tau \right\}$
    and $\hat{\cS}^+ = \left\{ j \mid w_j \geq \tau^+ \right\}$.
    These selections ensure the following FDR controls:
    \begin{equation}
        \mfdr\big[ \hat{\cS} \big] \leq q
        ,\qquad
        \fdr\big[ \hat{\cS}^+ \big] \leq q
    \end{equation}
\end{theorem}
Even if only the \textit{knockoff+} method truly control the FDR,
using the threshold $\tau$ improves the power and gives reasonable FDR,
in the same way the BH procedure usually controls the FDR even when the tests are not independent.

\section{Bottlenecks}\label{sec:kb}

Despite the nice theoretical guarantees on the $\fdr$ control that the knockoff procedure proposes,
two bottlenecks hurt its performances in the high-dimensional context.

\subsection{Knockoffs construction}\label{subsec:bot_sdp}

In the gaussian setting,
knockoff features are sampled according to the distribution~\ref{eq:conditional_gaussian_knockoffs}.
The control guarantees hold for any $\bs \in \R^p$ such that the covariance matrix $\Omega$ is semidefinite positive.
\begin{proposition}\label{prop:omega_psd}
    Let $\Omega = \begin{bmatrix}
        \Sigma & \Sigma - \diag \bs\\
        \Sigma - \diag \bs & \Sigma
    \end{bmatrix}$.
    Then $\Omega \succeq \0_{p \times p}$ if and only if $2\Sigma \succeq \diag \bs \succeq \0$.
\end{proposition}
\begin{proof}
    Note $D = \diag\bs$.
    The Schur complement $\Omega_{/\Sigma}$ (see Appendix~\ref{sec:schur_complement}) is given by
    $\Omega_{/\Sigma} = 2D - D\Sigma^{-1}D$.
    From this, we get that $\Omega \succeq \0$ is equivalent to $\Omega_{/\Sigma} \succeq \0$.
    Define $M$ and its Schur complements as follows
    \begin{equation*}
        M = \begin{bmatrix}
            2D & D\\
            D & \Sigma
        \end{bmatrix}
        ,\qquad\qquad
        \begin{cases*}
            M_{/2D} = \Sigma - \frac{1}{2}D\\
            M_{/\Sigma} = 2D - D\Sigma^{-1}D
        \end{cases*}
    \end{equation*}
    Finally, $\Omega_{/\Sigma} = M_{/\Sigma}$ is p.s.d.\ if and only if both $\Sigma - \frac{1}{2}D$ and $D$ are p.s.d.
\end{proof}
Even if we could use \emph{any} $\bs$ satisfying this constraint,
but some solutions lead to a better statistical power.
As will be shown in Chapter~\ref{ch:sdp},
finding good solutions amounts to solving a SDP which becomes intractable when $p$ is large.

\subsection{Statistics computation}\label{subsec:bot_stats}

The computation of the statistics presented in Section~\ref{sec:ksc}
on the concatenated feature matrix $\big[ X,\, \tX \big]$ may also become an obstacle when $n$ and $p$ grow.
This comes from the fact that most estimators have hyper-parameters that need to be cross-validated.
The cross-validation step may multiply the training time by several order of magnitudes.
On top of that, many estimators have a training time complexity that is more than linear in the number of features.

\bigbreak
These two bottlenecks make the use of the knockoff procedure
virtually impossible when $p$ is more than a few thousands.
The following chapters tackle these two issues by proposing
a coordinate ascent algorithm, low-rank approximations
and statistics that are fast to compute.

\section{Python implementation}\label{sec:python_implementation}

Implementations of the knockoff filter framework were provided by~\cite{fixed_x_knockoffs, model_x_knockoffs}
(along with other coauthors) in the languages R and MATLAB\footnote{
    The R package page can be found at
    \href{https://cran.r-project.org/web/packages/knockoff/index.html}{this address}.
    Sources for both R and MATLAB are stored in this
    \href{https://github.com/msesia/knockoff-filter}{GitHub repository}.
}.
To the best of our knowledge, no public and unified Python implementation is presently available.
As Python is a very popular language in the machine learning community,
and as it keeps growing year after year,
we decided to make a Python implementation\footnote{
    The implementation can be found at
    \href{}{this address}
} that contains fundamental components and that may be easily extended
depending on the needs of potential users.

The implementation is deliberately compatible with the Scikit-Learn ecosystem~\citep{sklearn},
so that estimators may be combined and used in a pipeline object.
It provides 3 main components,
namely a knockoffs generator \code{KnockoffsGenerator},
statistics computation utility functions,
and a selector \code{BaseKnockoffsSelector}.
The constituent \code{KnockoffsGenerator} subclasses
\code{BaseEstimator} and \code{TransformerMixin} from Scikit-Learn,
and \code{BaseKnockoffsSelector} subclasses
and \code{BaseEstimator} and \code{SelectorMixin}.
They implement \code{fit} and \code{transform} methods which are intuitive and follow Scikit-Learn semantics.

Components whose speed is crucial are written in Cython~\citep{cython}
and take advantage of the highly optimized libraries BLAS and LAPACK,
that are available in Cython through a SciPy wrapper.
It is for instance the case of the coordinate ascent algorithm detailed in Chapter~\ref{ch:sdp},
allowing to construct knockoffs efficiently.
\begin{calgorithm}
\begin{minted}[linenos]{python}
selector = SimpleKnockoffsSelector(
    GaussianXKnockoffs(),
    z_to_w_statistics(estimator_statistics(estimator=LassoCV())),
    alpha=0.2,
)

selector.fit(X, y)
print(f'Selected features: {selector.mask_}')
\end{minted}
\caption{
    Example of knockoffs usage
}\label{code:python_knockoffs}
\end{calgorithm}

\bigbreak
In the following chapters,
we present the ideas and algorithms that we implemented to make this Python toolkit scalable to large dimensions.
