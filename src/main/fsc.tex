\chapter{Fast statistics computation}\label{ch:fsc}

As described in Chapter~\ref{ch:knockoffs},
the computation of statistics on the concatenated feature matrix
is a key step of the knockoff selection process.
We discussed in Section~\ref{sec:ksc} the fact that a large variety of statistics control the FDR,
provided that the \textit{flip-sign} property~\ref{def:flip_sign} is satisfied.
However, the power is highly impacted depending on how suitable for the data the statistics are.

In this chapter, we note $X \in \R^{n \times p}$ the original design matrix,
and we suppose that we sampled valid knockoff features $\tX\in \R^{n \times p}$
(respecting conditions~\ref{itm:cond_swap} and~\ref{itm:cond_indep}).
The target vector is noted $\yy \in \R$.

\section{Lasso is good but slow}\label{sec:}

Experimentally, cross-validating a regressor (or classifier) against the data $\big[ X, \tX \big]$,
keeping its coefficients in absolute value,
and aggregating them as described in Subsection~\ref{subsec:ksa} yields excellent results.
Most classical regression models scale nicely as $p$ increases;
$\cO\left( n \cdot p \right)$ for Lasso~\citep{lasso} and logistic regression~\citep{logistic_regression},
$\cO\left( n \cdot p^2 \right)$ for support vector machines (SVM)~\citep{svm}.
In practice, the Lasso is observed to produce the highest statistical power in many cases~\citep{model_x_knockoffs}.
But all of these models have hyper-parameters that need to be tuned,
usually through cross-validation,
and with a naive grid search.
Tuning the estimator tends to lead to a higher power than picking ad hoc hyper-parameters,
but it increases the time complexity of the training phase by a lot.
Figure~\ref{fig:lasso_times} shows the time required to tune the penalty coefficient of a Lasso model,
using the algorithm LARS~\citep{lars}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth, height=0.5\linewidth]{figures/lasso_cv_times.pdf}
    \caption{
        Convergence time to cross-validate the parameter $\lambda$ of a Lasso model,
        as a function of the feature space dimension $p$ (original + knockoff features).
        Benchmark done in Python using the class \code{LassoCV} from \code{sklearn.linear\_model}.
        The data is the one described in Section~\ref{sec:genetic_data},
        and contains $2\,695$ samples.
    }
    \label{fig:lasso_times}
\end{figure}
For $5\,000$ features (in total, with the knockoffs),
it already needs $\approx 12$ minutes to compute the statistics.

Many datasets, like the ones we will consider for experiments in Chapter~\ref{ch:exp},
contain dozens of thousands of features.
For this reason, we try to develop faster techniques and statistics in the following sections
(potentially at the cost of a lower power).

\section{Multi-stage procedures}\label{sec:multi_stage}

A natural way to address the high number of features is to split the selection into several steps.
If $p$ is high,
you may want to reduce the number of features by cutting
a large proportion of them with a cheap and fast procedure first
(for example univariate regression),
and use more sophisticated methods afterwards on the remaining set of features.
However, methods that control the FDR, like BH, BY, and the knockoffs, are not guaranteed to work with this
feature truncation approach anymore.
A modification of the BH procedure is proposed by~\cite{multi_stage_fdr}
to address the issue of this multi-stage evaluation.
They prove that their process effectively controls the FDR on the entire set of features, as desired.

In our Python implementation mentioned in Section~\ref{sec:python_implementation},
we propose a \code{MultiStepsSelector} class to do that kind of multi-steps selections.
We are well-aware that no theoretical guarantee exists in the case of the knockoff procedure.
But we observed on synthetic data that in practice the false discovery proportions obtained
were not too far away from the target value.
Making a theoretical analysis will be the subject of future work.

\section{Sparse naive Bayes}\label{sec:snb}

\input{main/snb}

\section{Other fast statistics}\label{sec:a}

Remember the Lasso Signed Max (LSM) statistics defined in~\ref{eq:lsm}.
\begin{equation}\label{eq:lsm}
    z_j = \sup\{\lambda \mid \hat{\bbeta}_j(\lambda) \neq 0\}
    ,\qquad\quad
    \hat{\bbeta}(\lambda) =
        \underset{\bb}{\argmin}\;
        \frac{1}{2}\big\lVert \yy - \big[ X,\, \tX \big]\bb \big\rVert_2^2 + \lambda\norm{\bb}_1
\end{equation}
The more significant the feature $j$ is,
the higher the first value of $\lambda$ such that it enters the model will be.
That kind of statistics relying on a penalty could be used for other models than the Lasso,
and be potentially easier to compute.
Another possibility is to replace the $\ell_1$ penalty by an $\ell_0$ one.
We find that when this is done for bernoulli and gaussian naive Bayes,
and for centroid classifiers (depicted in Subsection~\ref{subsubsec:centroids}),
the statistics $z_j = \sup\{\lambda \mid \hat{\bbeta}_j(\lambda) \neq 0\}$
can be computed in closed-form.

As a matter of example, for the $\ell_2$ centroids penalized with the $\ell_0$ norm,
the optimization problem is the following one.
\begin{equation}\label{eq:penalized_centroids_averages}
    \hat{\bbeta}(\lambda) = \argmin_{( \btheta^+,\, \btheta^-)}
        \frac{1}{n^+} \sum_{i \in \cI^+} \big\lVert \xx_i - \btheta^+ \big\rVert^2_2
        + \frac{1}{n^-} \sum_{i \in \cI^-} \big\lVert \xx_i - \btheta^- \big\rVert^2_2
        + \lambda \norm{\btheta^+ - \btheta^-}_0
\end{equation}
We find that
\begin{align*}
    z_j = \frac{1}{2}(\bar{\xx}^+_j - \bar{\xx}^-_j)^2
\end{align*}
There is also a very cheap way to compute such values with $\ell_1$ norms in~\ref{eq:penalized_centroids_averages},
with a $\ell_0$ penalization of $\lambda$.

\bigbreak
All these fast heuristics are available in our Python impplementation.
They obviously lead to a lower statistical power than more expressive models in general.
We show experiments with these statistics in Chapter~\ref{ch:exp}.
