\chapter{Fast statistics computation}\label{ch:fsc}

As described in Chapter~\ref{ch:knockoffs},
the computation of statistics on the concatenated feature matrix
is a key step in the knockoff selection process.
As it has been discussed, a large variety of statistics control the FDR,
provided that the \textit{flip-sign} property~\ref{def:flip_sign} is satisfied.
However, the power is highly impacted depending on how suitable the statistics are for the data.
We also suppose that the knockoffs were generated respecting the conditions\dots.
Experimentally, it appears that for most tasks the coefficients of a cross-validated Lasso

In this chapter, we note $X \in \R^{n \times p}$ the original design matrix,
and we suppose that we sampled valid knockoff features $\tX\in \R^{n \times p}$.

\section{Lasso is good but slow}\label{sec:}

Experimentally, cross-validating a regressor (or classifier) against the data $\big[ X, \tX \big]$,
keeping its coefficients in absolute value,
and aggregating them as described in Subsection~\ref{subsec:ksa} yields excellent results.
Most classical regression models scale pleasantly as $p$ increases;
$\cO\left( n \cdot p \right)$ for Lasso and logistic regression,
$\cO\left( n \cdot p^2 \right)$ for SVMs.
But when the number of features is several thousands or more,

\section{Multi-stage procedures}\label{sec:multi_stage}

A natural way to address the high number of features is to split the selection into several steps.
If $p$ is high,
you may want to reduce the number of features by cutting a large proportion with a cheap and fast procedure first,
and use more sophisticated methods afterwards on the remaining set of features.
Multi-stage procedure~\cite{multi_stage_fdr}

\section{Fast statistics}\label{sec:a}

From now, only binary classification problems will be considered, where the label $y$ takes values
in $\left\{ \cC^+,\, \cC^- \right\}$.

\section{Sparse naive Bayes}\label{sec:snb}

\input{main/snb.tex}
