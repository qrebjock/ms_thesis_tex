\chapter{Fast statistics computation}\label{ch:fsc}

The computation of statistics is a key step in the knockoff selection process described in section~\ref{ch:knockoffs}.
As it has been discussed, a large variety of statistics control the FDR,
provided that the \textit{flip-sign} property~\ref{def:flip_sign} is satisfied.

However, the power can drastically change depending on how good the statistics are.
We also suppose that the knockoffs were generated respecting the conditions\dots.
Experimentally, it appears that for most tasks the coefficients of a cross-validated Lasso

In this chapter, we note $X \in \R^{n \times p}$ the original design matrix,
and we suppose that the distribution $\tcX \mid \cX$ could be computed.
Let $\tX \in \R^{n \times p}$ be sampled from that knockoff distribution.

\section{Lasso is good but slow}\label{sec:}

Experimentally, cross-validating a regressor (or classifier) against the data $\big[ X, \tX \big]$,
keeping its coefficients in absolute value,
and aggregating them as described in~\ref{subsec:ksa} yields excellent results.
Most classical regression models scale pleasantly as $p$ increases;
$\cO(n \cdot p)$ for Lasso and logistic regression,
$\cO(n \cdot p^2)$ for SVMs.
But when the number of features is several thousands or more,

\section{Multi-stage procedures}\label{sec:multi_stage}

A natural way to address the high number of features is to split the selection into several steps.
If $p$ is high,
you may want to reduce the number of features by cutting a large proportion with a cheap and fast procedure first,
and use more sophisticated methods afterwards on the remaining set of features.
Multi-stage procedure~\cite{multi_stage_fdr}

\section{Fast statistics}\label{sec:a}

