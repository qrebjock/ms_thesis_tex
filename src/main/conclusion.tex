\cleardoublepage
\chapter*{Conclusion}
\markboth{Conclusion}{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In this master thesis,
we investigated high-dimensional feature selection
in the gaussian knockoffs framework setting~\citep{model_x_knockoffs}.
In high dimension, nearly all the steps of the selection procedure become challenging,
and in particular the construction of the knockoff features,
as well as the computation of statistics associated to each aggregated feature.

Building knockoffs inducing a great statistical power relies on the optimization of a semidefinite program.
Hopefully, the structure and the simple bounds of this SDP allow us
to perform coordinate ascent (with log-barrier) which is guaranteed to converge in this case.
On top of that, a low-rank covariance approximation speeds up the computations
and cuts down the memory usage by several orders of magnitude.
Experiments show that a reasonably high power is preserved by this approach.
Despite the fact that the Lasso constitutes a very strong baseline,
we presented other statistics that scale much better,
at the cost of a lower power.

We provided a Python package implementing all these algorithms,
and took advantage of the Cython acceleration (along with BLAS and LAPACK)
to speed up critical parts of the selection pipeline.
We demonstrated the effectiveness of these algorithms on a genetic dataset that contains too many features
for usual methods to work in a reasonable amount of time.

From there, many questions and prospects remain open.
For the SNB model,
it would be valuable to find a way to compute the whole path of solutions,
rather than optimizing them from scratch for every sparsity level.
This would allow to tune SNB very quickly,
in the same way the Lasso can be cross-validated very efficiently with LARS~\citep{lars}.
Regarding the low-rank coordinate ascent algorithm,
we are working on fixing the numerical instabilities in the
$\cO\left( n_\text{iters} \cdot p \cdot k^2 \right)$ version with more robust rank-1 updates
(such as $LDL^\top$).
This algorithm could also take advantage of GPU acceleration
as it relies heavily on matrix multiplications.
Finally, false discovery rate control in high dimensions finds applications everywhere,
and we would like to experiment it on a wider range of datasets,
such as fMRI\@.
