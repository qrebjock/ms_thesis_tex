\cleardoublepage
\chapter*{Introduction}
\markboth{Introduction}{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Feature selection is an active area of research in statistics and machine learning
aiming, among others, at making models more interpretable.
The idea is to identify the most relevant observed covariates that best explain a phenomenon.
Often, one is interested in selecting as many pertinent features (true positives) as possible,
that is, maximizing true discoveries,
while maintaining the number of false positives under a certain threshold.
This problem is called false discovery rate control and finds applications in many sciences,
such as biology, economy, medicine and many others.
This is why a multitude of techniques have been developed to this end,
the most widely used being the one proposed by~\cite{bh}.
However, in the past two decades, acquiring massive amounts of data has become increasingly cheaper
and it is now frequent that datasets contain several dozens of thousands of features.
A common practice is to measure as many variables as possible,
and figure out afterwards which of them prove to be valuable.
On top of that, measuring covariates is often simpler than labeling samples with the right category,
as the latter requires human efforts and can hardly be automatized.
For that reason, many datasets have more features than samples
and it is especially the case in some fields like biology~\citep{statistical_inference_genome},
where genomic and fMRI data contain a huge amount of features but a relatively low number of patients.
In this high dimension setup, feature selection and false discovery rate control are particularly challenging.
The Benjamini–Hochberg procedure requires estimates of independent $p$-values for each feature,
which rapidly becomes hard when the dimension increases,
or may even be impossible.

Recently~\citet{fixed_x_knockoffs} introduced the knockoffs framework,
which is a set of procedures capable of performing feature selection,
and that are proven to theoretically control the false discovery rate under mere assumptions.
The key idea is to build a knockoff (that is, fake) feature for each authentic feature,
and compare their significance against a statistical model.
This framework was revisited and extended to a more general inference setting~\citep{model_x_knockoffs},
allowing notably to perform feature selection in the high dimension case,
that is when the number of features is larger than the number of samples.
The generality of this selection procedure
and the fact that many statistical models can be plugged in make it particularly appealing.
However, the construction of the knockoff features and the computation of test statistics
are two costly steps requiring to solve optimization problems whose size grow with the number of features.
Because of these two bottlenecks,
the knockoff framework can hardly be employed in high dimension in practice despite the theoretical guarantees.

In this work, we focus on the high-dimensional knockoff setting and propose techniques
to alleviate the bottlenecks mentioned above.
Constructing quality knockoffs relies on solving a SDP problem
which quickly becomes intractable as the dimension increases.
We propose a coordinate ascent approach with low-rank approximations
that both theoretically and experimentally scale well with the number of features.
Regarding test statistics, recent feature selection models~\citep{sparse_naive_bayes, sparse_center_classifiers}
empirically get close to the performances of the Lasso~\citep{lasso} at very cheap computational cost.
In the end, we are able to apply the knockoff framework
in a linear time complexity with respect to the number of dimension.

This master thesis is organized as follows.
Chapter~\ref{ch:fs} introduces the concepts of feature selection and
false discovery rate control, along with a few techniques that are usually employed.
Chapter~\ref{ch:knockoffs} details the knockoff selection procedure developed by Barber-Candès
and its principal theoretical guarantees.
In Chapter~\ref{ch:sdp},
we focus on the knockoff features constitution part
and propose a log-barrier coordinate ascent algorithm to solve the main bottleneck of the construction.
We show that low-rank covariance approximations considerably speed up the calculations.
Chapter~\ref{ch:fsc} addresses the problem of statistics computation,
and presents a sparse version of naive Bayes, among other things.
Finally, in Chapter~\ref{ch:exp} we detail experiments performed on genetic data and prove the scalability
and efficiency of our methods.
