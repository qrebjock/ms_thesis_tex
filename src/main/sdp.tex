\chapter{SDP}\label{ch:sdp}

In the case of gaussian knockoffs,
or more generally when we only impose
$\big[ X, \tX \big]_{\swap\left( S \right)}$
to have the same first two moments as
$\big[ X, \tX \big]$
for any subset
$S \subseteq \left\{ 1, \dots, p \right\}$,
rather than the same distribution~\ref{itm:cond_swap},
$\tX$ can be sampled from a normal distribution $\cN\left( \bupsilon,\, \Upsilon \right)$ whose parameters
$\bupsilon,\, \Upsilon$ are formulated in~\ref{eq:conditional_gaussian_knockoffs}.
These formulas are valid for any vector $\bs \in \R^p$ such that $\Omega$
is indeed a covariance matrix (semidefinite positive).
\begin{equation*}
    \Omega = \begin{bmatrix}
        \Sigma & \Sigma - \diag \bs\\
        \Sigma - \diag \bs & \Sigma
    \end{bmatrix}
    \succeq \0_{2p \times 2p}
\end{equation*}
As shown in proposition~\ref{prop:omega_psd},
this matrix is semidefinite positive if and only if $\0_{p \times p} \preceq \diag \bs \preceq 2 \Sigma$.
The first inequality clearly holds if all the entries of $\bs$ are positive.
The second one is trickier.

In this chapter, we show why the choice of $\bs$ is important and how to efficiently find a good one
in the high dimensional setting.
In the remaining, we suppose that we know $\Sigma$, even though in practice the covariance would be
estimated $\hat{\Sigma}$.

\section{Equi-correlated knockoffs}\label{sec:equi}

\subsubsection{A cheap solution}

For any psd matrix $A \in \R^p$, $A + \alpha \cdot \mI$ has the same eigenvalues as $A$, but shifted by $\alpha$.
It gives a fast and simple way to find a suitable $\bs$.
\begin{equation}\label{eq:equi}
    s_j = \min(2\lambda_{\min}(\covm), 1)
    \quad\text{ for all } 0 \leq j \leq p
\end{equation}
The problem of finding the smallest eigenvalue $\lambda_{\min}(\covm)$ can be solved efficiently.
Example.

\subsubsection{Why it is not desirable}

For large values of $p$,
the minimum eigenvalue of $\covm$ is very likely to be small,
unless $\covm$ has a substantially special structure.
Let us analyze briefly the covariance of $\big[ \cX; \tcX \big]$ to understand why it is not desirable.
\begin{equation*}
    \begin{cases}
        \cov(\cX_j,\, \tcX_{j'}) = \cov(\cX_j,\, \cX_{j'}) \text{ for all } j \neq j'\\
        \cov(\cX_j,\, \tcX_j) = \cov(\cX_j,\, \cX_j) - s_j \text{ for all } j
    \end{cases}
\end{equation*}
First, $\tcX$ has the same internal covariance has $\cX$,
and two distinct original and knockoff features have the same covariance as the one of the two original ones.
It makes the knockoff feature sufficiently close to the original features to fool them.
However, an original feature $j$ and its corresponding knockoff that is smaller the larger $s_j$ is.
If $s_j$ was $0$, $\cX_j$ couldn't be distinguished from $\tcX_j$ and the selection would suffer from a low power.

\section{SDP knockoffs}\label{sec:sdp}

This observation motivates us to maximize the entries of $\bs$,
while maintaining the inequality $\diag\bs \preceq \tcovm$.
This can be formulated in the optimization problem~\ref{eq:sdp}.
\begin{equation}\label{eq:sdp}
    \underset{\bs \in \R^p}{\argmax}\;\,
    \sum_{j = 1}^p s_j
    \qquad
    \text{subject to } \begin{cases}
        s_j \geq 0\text{ for all } j\\
        \diag \bs \preceq \tcovm
    \end{cases}
\end{equation}
This problem is a structured semidefinite program (SDP) and can efficiently be solved for small values of
$p$ by interior point methods~\cite{interior_point_method_sdp} for example.
For larger values of $p$, even first order methods like
SCS~\cite{sdp_scs} or alternating direction~\cite{sdp_admm}
quickly become intractable.
Even though the convergence speed depends a lot on $\covm$,
it experimentally appears that alternative methods have to be considered when $p > 1000$.

In order to reduce the computation time,
Barber-Cand√®s suggest to solve an approximated problem of~\ref{eq:sdp} in 2 steps that we describe below.
\paragraph*{Step 1.}
Pick an approximation $\Sigma_\text{approx}$ of $\Sigma$ and solve
\begin{equation}\label{eq:sdp_approx}
    \underset{\hat{\bs} \in \R^p}{\argmax}\;\,
    \sum_{j = 1}^p \hat{s}_j
    \qquad
    \text{subject to } \begin{cases}
        \hat{s}_j \geq 0\text{ for all } j\\
        \diag \hat{\bs} \preceq \tcovmapprox
    \end{cases}
\end{equation}
\paragraph*{Step 2.}
Solve the one dimensional maximization
\begin{equation}\label{eq:sdp_approx_1d}
    \underset{\gamma \in \R}{\argmax}\;\,\gamma
    \quad\text{subject to}\quad
    \diag\left( \gamma \cdot \hat{\bs} \right) \preceq 2\Sigma
\end{equation}
Finally, pick $\bs = \gamma \cdot \hat{\bs}$.

\begin{remark}
    Note that the two extreme options $\covmapprox = \mI$ and $\covmapprox = \Sigma$ yield
    the same solutions as solving equi-knockoffs~\ref{eq:equi} and full SDP knockoffs~\ref{eq:sdp} respectively.
\end{remark}

Solving~\ref{eq:sdp_approx_1d} in step 2 can be done very efficiently using bisection as it is a one-dimensional SDP\@.
The optimization problem~\ref{eq:sdp_approx} is however the same as the one in~\ref{eq:sdp},
except for the approximation $\covmapprox$.
To speed up the computations,
$\covmapprox$ can be chosen to be a block-diagonal matrix of $k$ blocks.
The maximization then reduces to $k$ smaller SDPs for which the solutions can be found more efficiently.
These could even be distributed in several computation nodes.
Picking $\covmapprox$ is a compromise between available computation time and eventual power of the procedure.
There is a priori no ideal way to to it.
